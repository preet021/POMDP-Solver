# This is just like three_state.pomdp, except that at every step there
# is a 0.1 probability of transitioning to s3, which is a zero-reward
# absorbing state.  In other words, the system always terminates.  This
# makes it easier to solve with some of the ZMDP search strategies such
# as RTDP and LRTDP that might otherwise run off on an infinite-length
# (or anyway, impractically long) trial.
#
# Copyright (c) 2002-2006, Trey Smith.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you
# may not use this file except in compliance with the License. You may
# obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License.

discount: 0.9
values: reward
states: s0 s1 s2 s3
actions: a0 a1 a2 obs
observations: p0 p1 p2 pdone

start: 
0.3333333333 0.33333333333 0.33333333333 0.0

T:a0
0.7 0.1 0.1 0.1
0.1 0.7 0.1 0.1
0.1 0.1 0.7 0.1
0.0 0.0 0.0 1.0

T:a1
0.7 0.1 0.1 0.1
0.1 0.7 0.1 0.1
0.1 0.1 0.7 0.1
0.0 0.0 0.0 1.0

T:a2
0.7 0.1 0.1 0.1
0.1 0.7 0.1 0.1
0.1 0.1 0.7 0.1
0.0 0.0 0.0 1.0

T:obs
0.7 0.1 0.1 0.1
0.1 0.7 0.1 0.1
0.1 0.1 0.7 0.1
0.0 0.0 0.0 1.0

O:a0
1 0 0 0
1 0 0 0
1 0 0 0
0 0 0 1

O:a1
1 0 0 0
1 0 0 0
1 0 0 0
0 0 0 1

O:a2
1 0 0 0
1 0 0 0
1 0 0 0
0 0 0 1

O:obs
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

R:a0     : s0    : * : * 3
R:a0     : s1    : * : * 0
R:a0     : s2    : * : * 0

R:a1     : s0    : * : * 0
R:a1     : s1    : * : * 3
R:a1     : s2    : * : * 0

R:a2     : s0    : * : * 0
R:a2     : s1    : * : * 0
R:a2     : s2    : * : * 3

R:obs    : *     : * : * 1

R:*      : s3    : * : * 0
